<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
  <title>Ian Carrasco | A blog about CS + Neuro, Music, and Design</title>
  <meta name="generator" content="Jekyll v4.0.0" />
  <meta property="og:title" content="Ian Carrasco" />
  <meta property="og:locale" content="en_US" />
  <meta name="description" content="A blog about CS + Neuro, Music, and Design" />
  <meta property="og:description" content="A blog about CS + Neuro, Music, and Design" />
  <link rel="canonical" href="http://localhost:4000/" />
  <meta property="og:url" content="https://iancarras.co" />
  <meta property="og:site_name" content="Ian Carrasco" />
  <script type="application/ld+json">
  {"description":"A blog about CS + Neuro, Music, and Design","@type":"WebSite","url":"http://localhost:4000/","name":"Ian Carrasco","headline":"Ian Carrasco","@context":"https://schema.org"}</script>
  <!-- End Jekyll SEO tag -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: [
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js",
        "a11y/accessibility-menu.js"
      ],
      jax: ["input/TeX", "output/CommonHTML"],
      TeX: {
        extensions: [
          "AMSmath.js",
          "AMSsymbols.js",
          "noErrors.js",
          "noUndefined.js",
        ]
      }
    });
  </script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ian Carrasco" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ian Carrasco</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a><a class="page-link" href="/projects/">Projects</a><a class="page-link" href="/work/">Work</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hebbian Learning | NeuroAI #1</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-01-15T10:05:52-08:00" itemprop="datePublished">Jan 15, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="/assets/pyramidal.jpg"><img src="/assets/pyramidal.jpg" alt="" /></a></p>

<p class="image-caption">Illustration of a pyramidal neuron in the cerebral cortex by Ramon y Cajal. <br /><em>Courtesy of the Cajal Institute</em></p>

<h1 id="introduction-and-considerations">Introduction and Considerations</h1>

<p>Understanding how learning occurs in the brain is a key question in neuroscience, alongside those of memory and visual processing. This is a particularly difficult problem, as with many other inquires about the brain, since it can be analyzed at many different scales (molecules, individual neurons, circuits, behavior). In many cases this leads to varying hypotheses about the same process.</p>

<p>Given the emergent nature of brain structure, there seems to be a tight coupling between the aforementioned scales. Lets take an example of modeling the dynamics of a neural circuit. When observing a circuit’s dynamics, we are observing a process which is built upon assumptions of underlying structure which we would be naïve to not consider. Depending on the chosen working model of single neuron, a resulting model of circuit dynamics can be heavily influenced. Essentially, this comes down to recognizing when/where models are interconnected and understanding to some degree how assumptions can propagate within them.</p>

<p>Continuing on the theme of modeling, I want to classify two main approaches towards building systems with some resemblance to their biological counterparts.</p>

<p>The first is to analyze the structure and dynamics of a neural system and building a computational model consistent with that structure. The second is to build a model which can perform a non-trival task fairly well, and look for parallels at the computational and representational level (Check Marr) post facto. I think the first approach can be seen as a bottom up/emergent view and the second approach a top-down/distillation view.</p>

<p><strong><span class="orange">Bottom-Up/Emergent (Structure → Behavior)</span></strong></p>
<blockquote>
  <ol>
    <li>Attempt to model specific aspects of brain structure (connectivity patterns, cell types, etc.)</li>
    <li>Train on a given task and observe model’s behavior w.r.t human behavior</li>
  </ol>
</blockquote>

<p><strong><span class="orange">Top-Down/Distillation (Behavior → Structure)</span></strong></p>
<blockquote>
  <ol>
    <li>Identify a complex task humans are commonly able to solve</li>
    <li>Build a model using existing deep learning approaches to try and solve said task</li>
    <li>Observe model’s behavior with respect to human behavior, and draw parallels between ANN and brain structure using interpretability techiques and adversarial examples.</li>
  </ol>
</blockquote>

<!-- An common approach to trying to build systems which performa is to use existing machine learning opposed to identifying what processes occur in the brain (i.e. object detection, planning) and attempting to emulate high-level behavior. For example, we see this displayed in works looking at parallels between CNNs and visio-cortical processing. Where instead of trying modeling the visual cortex by trying to model cortical neurons and connectivity patterns directly, we use our standard deep CNN architectures and observe if there are computational properties that arise similar to the visual cortex, such as edge detection and higher level processing as a function of layer depth. [https://arxiv.org/pdf/2001.07092.pdf](https://arxiv.org/pdf/2001.07092.pdf) -->

<p>With this thought aside, we’ll first look at a computational account of learning and synaptic plasticity in the brain known as Hebbian Learning (HL). Then, we’ll draw connections to some classical and modern machine learning architectures inspired by HL including Hopfield Nets, Competing Hidden Units, and Differentiable Plasticity.</p>

<p>Throughout, the aim is to see how intuition about biological learning processes can be adapted into frameworks that are amenable for computation.</p>

<p><img src="/assets/hebb.png" alt="" /></p>

<h1 id="what-is-hebbian-learning">What is Hebbian Learning?</h1>
<p>At it’s core, hebbian learning states that if two neurons <strong><span class="green underlit">i</span></strong> (pre-synaptic) and <strong><span class="green underlit">j</span></strong> (post-synaptic) connected by a synapse, <strong><span class="green underlit">s</span></strong>, the synaptic strength can be seen as a product of the pre- and post- synaptic activity, typically within the range of a small time window.</p>

<p>From a causal perspective, timing is an important factor as neuron i has to fire just before neuron j to make a potential claim that i has a role in j’s firing, a case which under Hebb’s model would lead to a synapse strengthening.</p>

<p>On the other hand, if the activity of j tends to be uncorrelated to that of i (i.e. j firing before i) then we’d tend to see a weakening of the synapse. The timing of neural activity contributing to both strengthing <strong>and weakening</strong> is captured by a generalization of Hebb’s rule known as Spike Time Dependent Plasticity (STDP), and is a widely accepted explanation for long-term potentiation and depression of neuron populations.</p>

<h1 id="formalization-of-a-hebbian-learning-rule">Formalization of a Hebbian Learning Rule</h1>

\[\Delta w_{ij} = \alpha \cdot x_i \cdot x_j\]

<p>The Hebbian learning account provides an intuitive, experimentally backed framework for describing how synaptic weights change through time-correlated activity between neurons.</p>

<p>Synapse strengthened (weight adjustment) when post-synaptic response occurs in conjunction with pre-synaptic firing. Pre-synaptic neuron has to fire “just before” post-synaptic neuron so causality can be inferred.</p>

<p>Hebbian learning is an account of weight adjustment between neurons</p>

<p>Hebb supports a real-time learning mechanism, where the temporal association of signals is important to the efficacy of a synapse and corresponding learning mechanisms which adjust it</p>

<p>Non real-time learning error signals computed from system responses/order of inputs and outputs, but not the exact time of occurence of each input/output</p>

<p>Hebbian LTP is seen in conjunction with Hetero-synaptic long term depression, where the receiving neuron is activated with no activation of the sending neuron. Thus, the pre-synaptic function is a difference of its activation and current synaptic weight</p>

<p>same signs → strengthen synapse</p>

<p>opposite signs → weaken synapse</p>

<h3 id="hopfield-networks">Hopfield Networks</h3>

<h3 id="krotov-et-al-local-neuronal-error">Krotov et al. Local Neuronal Error</h3>

<h3 id="differentiable-plasticity">Differentiable Plasticity</h3>


  </div><a class="u-url" href="/2020/01/15/hebbian.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ian Carrasco</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ian Carrasco</li><li><a class="u-email" href="mailto:iansee.c@gmail.com">iansee.c@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/IanCarrasco"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">IanCarrasco</span></a></li><li><a href="https://www.twitter.com/ia_n_ai"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">ia_n_ai</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog, resume, and archive of things I&#39;m interested in and have worked on.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
