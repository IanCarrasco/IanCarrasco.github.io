<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-24T19:47:08-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ian Carrasco</title><subtitle>A blog, resume, and archive of things I'm interested in and have worked on.</subtitle><entry><title type="html">Introducing the NeuroAI Series</title><link href="http://localhost:4000/2020/01/15/intronai.html" rel="alternate" type="text/html" title="Introducing the NeuroAI Series" /><published>2020-01-15T10:05:52-08:00</published><updated>2020-01-15T10:05:52-08:00</updated><id>http://localhost:4000/2020/01/15/intronai</id><content type="html" xml:base="http://localhost:4000/2020/01/15/intronai.html">&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/20/Rat_hippocampus_stained_with_antibody_to_NeuN_%28green%29%2C_myelin_basic_protein_%28red%29_and_DNA_%28blue%29.jpg&quot; alt=&quot;&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;image-caption&quot;&gt;NeuN Stain of a Rat Hippocampus&lt;/p&gt;
&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;In the coming posts, I’m going to break down some
of the foundational and recent work in neuroscience-inspired
AI or NeuroAI for short. I believe this field has a lot of 
promise in regards to building systems that have general 
intelligence and reasoning capabilities. In their current state, deep learning architectures under-represent
several of the structural and dynamical aspects of neurons and their corresponding networks seen in the brain. Some 
of these aspects include cell diversity, spatial organization, and neuromodulation.&lt;/p&gt;

&lt;p&gt;While taking the approach of applying known phenomena from neuroscience to AI can lead to interesting, novel developments, it
is still in its infancy. A more common approach that is to ask if existing mechanisms are biologically plausible. It could very well be the case that staying true to biological limitations may not be the proper approach. One such reason could be due to the fundamental differences in computation that occur across different substrates (human brains and computers). Continuing with this point, neuroscience should be viewed as a source of inspiration rather than a guide to follow strictly. As more developments occur,this dichotomy might change but only time will tell. I find this area to be particularly exciting as it makes one consider the assumptions made about the brain that influenced the design of artificial neural networks. Additionally and most importantly, it provides a basis to challenge these assumptions with new algorithms and architectures.&lt;/p&gt;

&lt;p&gt;The first post in the series will be about the principle of &lt;strong&gt;Hebbian Learning&lt;/strong&gt;. Which is colloquially known via the popular phrase
“neurons that fire together wire together”. We will first look at the principle from a neuroscience lens, then move into its
computational formulation by looking at a paper by Miconi, Clune, et al. (Uber AI), &lt;em&gt;Differentiable plasticity: training plastic neural networks with backpropagation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Stay Tuned.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Technological Inertia</title><link href="http://localhost:4000/world-view/2019/12/22/technological-inertia.html" rel="alternate" type="text/html" title="Technological Inertia" /><published>2019-12-22T10:05:52-08:00</published><updated>2019-12-22T10:05:52-08:00</updated><id>http://localhost:4000/world-view/2019/12/22/technological-inertia</id><content type="html" xml:base="http://localhost:4000/world-view/2019/12/22/technological-inertia.html">&lt;p&gt;With the start of the new decade I’ve been thinking
about what technologies might define it whether it be
(AR, AI, Edge Computing, 5G, etc.). When evaluating 
developments and trends in consumer tech and research, I found
myself continually thinking about the dynamics of how 
new technologies are adopted in society and make their way 
into the status quo. In particular, I came up with a 
concept I call &lt;em&gt;technological inertia&lt;/em&gt;, whose definition
might be trivial, but I felt succinctly captures an important 
consideration when ideating.&lt;/p&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;An idea has to be familiar enough that the learning/adoption curve is relatively
small such that it could be intially adopted by those who are comfortable with existing technological norms.&lt;/p&gt;

&lt;p&gt;However, the idea has to add enough value or novelty to existing solutions, that one would give up using pre-existing 
technology in order to use it.&lt;/p&gt;

&lt;p&gt;Consumers have inherent inertia, a tendency to stick with what works. When the balance of &lt;strong&gt;familiarity and innovation&lt;/strong&gt; of an idea is not level, this inertial force is greater than the “force”(marketing, innovation, design) of an idea. Thus, leading to minimal or low adoption. If this balance is maintained, an idea will have a much better chance at being adopted.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: The term is derived from inertia in classical mechanics, which is an object’s resistance to change or tendency to stay at rest.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="world-view" /><summary type="html">With the start of the new decade I’ve been thinking about what technologies might define it whether it be (AR, AI, Edge Computing, 5G, etc.). When evaluating developments and trends in consumer tech and research, I found myself continually thinking about the dynamics of how new technologies are adopted in society and make their way into the status quo. In particular, I came up with a concept I call technological inertia, whose definition might be trivial, but I felt succinctly captures an important consideration when ideating.</summary></entry></feed>