I"¬#<p><a href="/assets/pyramidal.jpg"><img src="/assets/pyramidal.jpg" alt="" /></a></p>

<p class="image-caption">Illustration of a pyramidal neuron in the cerebral cortex by Ramon y Cajal. <br /><em>Courtesy of the Cajal Institute</em></p>

<h1 id="neuroscientific-considerations">Neuroscientific Considerations</h1>
<h4 id="the-problem-of-scales">The Problem Of Scales</h4>

<p>Understanding how learning occurs in the brain is a key problem in neuroscience. Learning, along with other high level mechanisms (i.e. vision, planning), are difficult to address due to the different scales which they can be analyzed from (i.e. individual neurons -&gt; neural populations -&gt; observable behavior). In some cases, this leads to varying hypotheses about the same underlying process.</p>

<p>Given the emergent nature of brain structures, there is an inherent coupling between scales. For example, when modeling neural circuit dynamics, the resulting model can be influenced by assumptions of lower-level structure. One assumption in this case is the choice of single neuron model (i.e. Hodgkin-Huxley, Leaky Integrate-and-Fire, etc.) and such a choice could impose new constraints and/or unexpected behaviors. <strong>Essentially, this idea comes down to recognizing when models are interconnected and understanding how working assumptions can propagate across scales.</strong></p>

<p><a href="/assets/pyramidal.levels.png"><img src="/assets/levels.png" alt="" /></a></p>

<p class="image-caption">Levels of analysis in the brain <br /><em>Courtesy of Terrence Sejnowski</em></p>

<h4 id="two-accounts-of-neuro-ai-modeling">Two Accounts of Neuro-AI Modeling</h4>

<p>With the consideration of scales addressed, I classify two main approaches towards building computational models that share function(s) with their biological counterparts. This formalization is not exhaustive, but can be used to classify a good portion of neuro/ai papers.</p>

<p>The first is to <strong>analyze the structure and dynamics of a neural mechanism</strong> and then <strong>build a computational model consistent with that structure</strong>.</p>

<p>The second is to <strong>build a system which can perform an innately human task well</strong> and then <strong>identify parallels with the brain at computational and representational levels</strong> [1]. The first approach can be seen as a bottom up/emergent view, whereas the second approach a top-down/distillation view.</p>

<p><strong><span class="orange">Bottom-Up/Emergent View (Structure ‚Üí Behavior)</span></strong></p>
<blockquote>
  <ol>
    <li>Attempt to model specific aspects of brain structure (connectivity patterns, cell types, etc.)</li>
    <li>Train model on a given task and observe model‚Äôs behavior w.r.t human behavior</li>
  </ol>
</blockquote>

<p><strong><span class="orange">Top-Down/Distillation View (Behavior ‚Üí Structure)</span></strong></p>
<blockquote>
  <ol>
    <li>Identify a complex task humans are commonly able to solve</li>
    <li>Build a model using existing deep learning approaches to try and solve said task</li>
    <li>Observe model‚Äôs behavior with respect to human behavior, and draw parallels between model and brain structure after the fact using interpretability techiques (adversarial examples, activation maps, etc.)</li>
  </ol>
</blockquote>

<p>An instructive example of the bottom up view comes from the topic of studying backpropagation in the brain. Where it is common to see analaysis of synaptic information processing in the brain are observed and then applied to existing computational models. Lillicrap et al. [2], demonstrates this with an alternative to backpropagation called feedback alignment, which follows from evidence that there is no precise backward connectivity in the brain despite backpropagation requiring it. With consideration of biological constraints, the authors proposed a model which allows propagation of error signals to hidden layer neurons without an explicit backward pass (no requirement of backward connectivity patterns). This work, along with many others in the space [3] [4] [5], employ a similar approach of taking inspiration from neuroscience studies and adapting specific properties to existing architectures. The topic of biologically plausible techniques for error propagation will be looked at in a future post.</p>

<p>An example of the top-down approach to trying can best be seen with research of feature extractors in convolutional neural networks (CNNs). Particularly, looking at parallels between convolutional layers and visual cortex layers. Hubel and Wiesel [5] noted that cells in the primary visual cortex (V1) tend to respond to locally oriented edges within small receptive fields. Following this, the neurons in subsequent layers have signficant responses to more complex features (i.e. V4 responding to shape outlines) [6] Where instead of trying modeling the visual cortex by trying to model cortical neurons and connectivity patterns directly, we use our standard deep CNN architectures and observe if there are computational properties that arise similar to the visual cortex, such as edge detection and higher level processing as a function of layer depth. <a href="https://arxiv.org/pdf/2001.07092.pdf">https://arxiv.org/pdf/2001.07092.pdf</a></p>

<p>With these thoughts aside, we‚Äôll first look at a computational account of learning and synaptic plasticity in the brain known as Hebbian Learning (HL). Then, we‚Äôll draw connections to some classical and modern machine learning architectures inspired by HL including Hopfield Nets, Competing Hidden Units, and Differentiable Plasticity.</p>

<p><img src="/assets/hebb.png" alt="" /></p>

<h1 id="what-is-hebbian-learning">What is Hebbian Learning?</h1>
<p>At it‚Äôs core, hebbian learning states that if two neurons <strong><span class="green underlit">i</span></strong> (pre-synaptic) and <strong><span class="green underlit">j</span></strong> (post-synaptic) connected by a synapse, <strong><span class="green underlit">s</span></strong>, the synaptic strength can be seen as a product of the pre- and post- synaptic activity, typically within the range of a small time window (300ms).</p>

<p>From a causal perspective, timing is an important factor as neuron <strong><span class="green underlit">i</span></strong> has to fire just before neuron <strong><span class="green underlit">j</span></strong> to make a potential claim that i has a role in j‚Äôs firing, a case which under Hebb‚Äôs model would lead to a synaptic strengthening.</p>

<p>On the other hand, if the activity of j tends to be uncorrelated to that of i (i.e. j firing before i) then we‚Äôd tend to see a weakening of the synapse. The timing of neural activity contributing to both strengthing <strong>and weakening</strong> is captured by a generalization of Hebb‚Äôs rule known as Spike Time Dependent Plasticity (STDP), and is a widely accepted explanation for long-term potentiation and depression of neuron populations.</p>

<h1 id="formalization-of-a-hebbian-learning-rule">Formalization of a Hebbian Learning Rule</h1>

\[\Delta w_{ij} = \alpha \cdot x_i \cdot x_j\]

<p>The Hebbian learning account provides an intuitive, experimentally backed framework for describing how synaptic weights change through time-correlated activity between neurons.</p>

<p>Synapse strengthened (weight adjustment) when post-synaptic response occurs in conjunction with pre-synaptic firing. Pre-synaptic neuron has to fire ‚Äújust before‚Äù post-synaptic neuron so causality can be inferred.</p>

<p>Hebbian learning is an account of weight adjustment between neurons</p>

<p>Hebb supports a real-time learning mechanism, where the temporal association of signals is important to the efficacy of a synapse and corresponding learning mechanisms which adjust it</p>

<p>Non real-time learning error signals computed from system responses/order of inputs and outputs, but not the exact time of occurence of each input/output</p>

<p>Hebbian LTP is seen in conjunction with Hetero-synaptic long term depression, where the receiving neuron is activated with no activation of the sending neuron. Thus, the pre-synaptic function is a difference of its activation and current synaptic weight</p>

<p>same signs ‚Üí strengthen synapse</p>

<p>opposite signs ‚Üí weaken synapse</p>

<h3 id="hopfield-networks">Hopfield Networks</h3>

<h3 id="krotov-et-al-local-neuronal-error">Krotov et al. Local Neuronal Error</h3>

<h3 id="differentiable-plasticity">Differentiable Plasticity</h3>

<h3 id="references">References</h3>
<ol>
  <li>D. Marr, https://dspace.mit.edu/handle/1721.1/5782 (1976)</li>
  <li>T. Lillicrap  et al., <em>Random synaptic feedback weights support error backpropagation for deep learning</em> (2016)</li>
  <li>B. Lansdell et al. ,<em>Learning to solve the credit assignment problem</em> (2019)</li>
  <li>I. Jones et al. <em>Can Single Neurons Solve MNIST? The Computational Power of Biological Dendritic Trees</em> (2020)</li>
  <li>J. Hawkins et al., <em>A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex</em> (2019)</li>
</ol>

:ET